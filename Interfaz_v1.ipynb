{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rasieira/anaconda3/envs/murcielagos/lib/python3.8/site-packages/torchaudio/backend/utils.py:53: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os, glob, random, time, sys, pickle\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import librosa, librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# import torch.optim as optim\n",
    "import torch_optimizer as optim\n",
    "from torchvision import transforms, models, datasets\n",
    "# from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "# from torchlibrosa.augmentation import SpecAugmentation\n",
    "from torchaudio.transforms import AmplitudeToDB\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm\n",
    "# from tqdm.notebook import tqdm\n",
    "from functools import partial\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score, ConfusionMatrixDisplay, confusion_matrix\n",
    "from skimage.transform import resize\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import timm\n",
    "from timm.models.efficientnet import tf_efficientnet_b0_ns, tf_efficientnet_b1_ns, tf_efficientnet_b2_ns, tf_efficientnet_b3_ns, tf_efficientnet_b4_ns\n",
    "\n",
    "from os import walk\n",
    "from pandas import DataFrame\n",
    "from fastai.vision.widgets import *\n",
    "import zipfile\n",
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "import warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    arquitectura = 'resnet34',\n",
    "    num_clases = 7,\n",
    "    spec = {\n",
    "        'n_fft' : 3072, #2048 3072 #16384, #384000 // 2 // 512 * 16, # 512 * 2\n",
    "        'hop_length' : 768, #512, 768, 1024\n",
    "        'fmax' : 96000, #384000 // 2,  192000 // 2\n",
    "        'fmin' : 8000,\n",
    "        'tramo': 1.5\n",
    "    }\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_epoch(model, loader):\n",
    "#     model.eval()\n",
    "#     y_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         t = tqdm(loader)\n",
    "#         for i, sample in enumerate(t):\n",
    "#             input = sample['image'].float().to(device)\n",
    "#             bs = input.size(0)\n",
    "            \n",
    "#             output = model(input)\n",
    "#             y_pred.append(torch.sigmoid(output).detach().cpu().numpy())\n",
    "#         t.close()\n",
    "#     y_pred = np.vstack(y_pred)\n",
    "#     return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_inferencia(path):      \n",
    "    imgs=[]\n",
    "    paths=[]\n",
    "    for (dirpath, dirnames, filenames) in walk(path):\n",
    "        for t in filenames:\n",
    "            completo=path+'/'+t\n",
    "            paths.append(str(completo))\n",
    "\n",
    "    for a in paths:\n",
    "        y, sr = sf.read(a)\n",
    "        len_crop = int(args.spec['tramo'] * sr)\n",
    "\n",
    "        # Repetimos la señal si es menor y luego cortamos\n",
    "        while len(y)<len_crop:\n",
    "            y = np.concatenate([y, y])\n",
    "        y = y[0:len_crop]\n",
    "\n",
    "        f_min = args.spec['fmin']\n",
    "        f_max = args.spec['fmax']\n",
    "\n",
    "        # Frecuencias y hop_length proporcionales a la sr\n",
    "        if sr==192000:\n",
    "            n_fft= args.spec['n_fft'] // 2\n",
    "            hop_length = args.spec['hop_length'] // 2\n",
    "        if sr==256000:\n",
    "            n_fft= args.spec['n_fft'] * 2 // 3\n",
    "            hop_length = args.spec['hop_length'] * 2 // 3\n",
    "        if sr==384000:\n",
    "            n_fft= args.spec['n_fft']\n",
    "            hop_length = args.spec['hop_length']\n",
    "\n",
    "        # Banda de frecuencias a filtrar\n",
    "        frecuencias = np.arange(0, 1 + n_fft / 2) * sr / n_fft\n",
    "        indices_freq = (frecuencias >= f_min) & (frecuencias <= f_max)\n",
    "\n",
    "        # Reduce el paso a la mitad si sr==192000\n",
    "        img = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length, win_length=n_fft, window='hamming'))**2\n",
    "        img = librosa.power_to_db(img, ref=np.max)\n",
    "        img = img[indices_freq,:]\n",
    "        img = torch.from_numpy(img).unsqueeze(0)\n",
    "        imgs.append(img)\n",
    "    return {\"images\" : imgs,'paths' : paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Model():\n",
    "    model = timm.create_model(args.arquitectura[0], pretrained=True, num_classes=args.num_clases[0], in_chans=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pl = widgets.Output()\n",
    "out_pl2 = widgets.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_click_classify(change):\n",
    "    data=dataset_inferencia(fc.selected_path)\n",
    "    df = DataFrame (data['paths'],columns=['paths'])\n",
    "    out_pl.clear_output()\n",
    "    out_pl2.clear_output()\n",
    "    model_path=fc1.selected_path+'/'+fc1.selected_filename\n",
    "    model=Model()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "#     with torch.no_grad():\n",
    "#         for i in data['images']:\n",
    "#             input = i.float().to(device)\n",
    "#             bs = input.size(0)\n",
    "#             output = model(input)\n",
    "#             y_pred.append(torch.sigmoid(output).detach().cpu().numpy())\n",
    "#     y_pred = np.vstack(y_pred)\n",
    "    with out_pl: display(df)\n",
    "    with out_pl2:display(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FileChooser('../')\n",
    "\n",
    "# Print the selected path, filename, or both\n",
    "if fc.selected_path is not None:\n",
    "    print(fc.selected_path)\n",
    "if fc.selected_filename is not None:\n",
    "    print(fc.selected_filename)\n",
    "if fc.selected is not None:\n",
    "    print(fc.selected)\n",
    "\n",
    "# Change defaults and reset the dialog\n",
    "fc.default_path = '../'\n",
    "fc.reset()\n",
    "\n",
    "# Shorthand reset\n",
    "fc.reset(path='../')\n",
    "\n",
    "# Change hidden files\n",
    "fc.show_hidden = True\n",
    "\n",
    "# Show or hide folder icons\n",
    "fc.use_dir_icons = True\n",
    "\n",
    "# Switch to folder-only mode\n",
    "fc.show_only_dirs = True\n",
    "\n",
    "# Change the title (use '' to hide)\n",
    "fc.title = '<b>Selecciona el path</b>'\n",
    "\n",
    "fc1 = FileChooser('../')\n",
    "\n",
    "# Print the selected path, filename, or both\n",
    "if fc1.selected_path is not None:\n",
    "    print(fc1.selected_path)\n",
    "if fc1.selected_filename is not None:\n",
    "    print(fc1.selected_filename)\n",
    "if fc1.selected is not None:\n",
    "    print(fc1.selected)\n",
    "\n",
    "# Change defaults and reset the dialog\n",
    "fc1.default_path = '../'\n",
    "fc1.reset()\n",
    "\n",
    "# Shorthand reset\n",
    "fc1.reset(path='../')\n",
    "\n",
    "# Change hidden files\n",
    "fc1.show_hidden = True\n",
    "\n",
    "# Show or hide folder icons\n",
    "fc1.use_dir_icons = True\n",
    "\n",
    "# Switch to folder-only mode\n",
    "fc1.show_only_dirs = False\n",
    "\n",
    "# Change the title (use '' to hide)\n",
    "fc1.title = '<b>Selecciona el Modelo (.bin)</b>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "btn_run = widgets.Button(description='Sacar Predicciones')\n",
    "btn_run.on_click(on_click_classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c75e0f5eb0345e5ace2c75160913ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(FileChooser(path='..', filename='', title='HTML(value='<b>Selecciona el path</b>')', show_hidde…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VBox([fc,fc1, out_pl,out_pl2,btn_run])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
